# -*- coding: utf-8 -*-
"""Matrix Factorization_TravelDataset.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1_5aBU70jHZtfk8I549NrHUcPw6EQe7VK
"""

#library imports
import numpy as np
import pandas as pd
from collections import Counter
from sklearn.model_selection import train_test_split
from scipy import sparse

# Jovian Commit Essentials
# Please retain and execute this cell without modifying the contents for `jovian.commit` to work
!pip install jovian --upgrade -q
import jovian

"""**Import google drive**"""

from google.colab import drive
drive.mount('/content/drive')

"""Dataset"""

travel_ratings_df = pd.read_csv("/content/drive/MyDrive/GCN_TripRecommendation/datasets/yelp_training_set_review.csv")
travel_ratings_df.shape
print(travel_ratings_df.head())

"""Getting rid of the ratings that are not present & make the datset to contain only the items that are rated >=4"""

travel_ratings = travel_ratings_df.loc[travel_ratings_df.stars >= 4].reset_index()[['user_id','business_id','stars']]
print(travel_ratings.shape)
travel_ratings.head()

"""Create train set, validation set,test set

Train-Valid Split
"""

train_df, valid_df = train_test_split(travel_ratings, test_size=0.1)
valid_df, test_df  = train_test_split(travel_ratings, test_size=0.5)

#resetting indices to avoid indexing errors in the future
train_df = train_df.reset_index()[['user_id', 'business_id', 'stars']]
valid_df = valid_df.reset_index()[['user_id', 'business_id', 'stars']]
test_df  = test_df.reset_index()[['user_id', 'business_id', 'stars']]

"""Training

**Encoding columns with continuous ids**

Because we'll be using PyTorch's embedding layers to create our user and item embeddings, we need continuous IDs to be able to index into the embedding matrix and access each user/item embedding.
"""

def encode_column(column):
    """ Encodes a pandas column with continous IDs"""
    keys = column.unique()
    key_to_id = {key:idx for idx,key in enumerate(keys)}
    return key_to_id, np.array([key_to_id[x] for x in column]), len(keys)

def encode_df(travel_df):
    """Encodes rating data with continuous user and business ids"""
    
    place_ids, travel_df['business_id'], num_places = encode_column(travel_df['business_id'])
    user_ids, travel_df['user_id'], num_users = encode_column(travel_df['user_id'])
    return travel_df, num_users, num_places, user_ids, place_ids

travel_df, num_users, num_places, user_ids, place_ids = encode_df(train_df)
print("Number of users :", num_users)
print("Number of places :", num_places)
travel_df.head()

valid_df, num_users, num_places, user_ids, place_ids = encode_df(valid_df)
print("Number of users :", num_users)
print("Number of places :", num_places)
valid_df.head()

test_df, num_users, num_places, user_ids, place_ids = encode_df(test_df)
print("Number of users :", num_users)
print("Number of places :", num_places)
test_df.head()

"""### Initializing user and item embeddings"""

def create_embeddings(n, K):
    """
    Creates a random numpy matrix of shape n, K with uniform values in (0, 5/K)
    n: number of items/users
    K: number of factors in the embedding 
    """
    return 5*np.random.random((n, K)) / K

5*np.random.random((5, 3))

"""Creating Sparse utility matrix"""

def create_sparse_matrix(df, rows, cols, column_name="stars"):
    """ Returns a sparse utility matrix""" 
    return sparse.csc_matrix((df[column_name].values,(df['user_id'].values, df['business_id'].values)),shape=(rows, cols))

travel_df, num_users, num_places, user_ids, place_ids = encode_df(train_df)
Y = create_sparse_matrix(travel_df, num_users, num_places)

# to view matrix
Y.todense()

"""### Making predictions"""

# Generates predicted values but dot product of user and item embeddings
def predict(df, emb_user, emb_places):
    """ This function computes df["prediction"] without doing (U*V^T).
    
    Computes df["prediction"] by using elementwise multiplication of the corresponding embeddings and then 
    sum to get the prediction u_i*v_j. This avoids creating the dense matrix U*V^T.
    """
    df['prediction'] = np.sum(np.multiply(emb_places[df['business_id']],emb_user[df['user_id']]), axis=1)
    return df

"""Cost"""

lmbda = 0.0002

# computes the loss here, subtracting the predicted vs actual values
def cost(df, emb_user, emb_places):
    """ Computes mean square error"""
    Y = create_sparse_matrix(df, emb_user.shape[0], emb_places.shape[0])
    predicted = create_sparse_matrix(predict(df, emb_user, emb_places), emb_user.shape[0], emb_places.shape[0], 'prediction')
    return np.sum((Y-predicted).power(2))/df.shape[0]

"""### Gradient Descent"""

def gradient(df, emb_user, emb_places):
    """ Computes the gradient for user and place embeddings"""
    Y = create_sparse_matrix(df, emb_user.shape[0], emb_places.shape[0])
    predicted = create_sparse_matrix(predict(df, emb_user, emb_places), emb_user.shape[0], emb_places.shape[0], 'prediction')
    delta =(Y-predicted)
    grad_user = (-2/df.shape[0])*(delta*emb_places) + 2*lmbda*emb_user
    grad_places = (-2/df.shape[0])*(delta.T*emb_user) + 2*lmbda*emb_places
    return grad_user, grad_places

import matplotlib.pyplot as plt

# for optimzation we use the gradient descent.
def gradient_descent(df, emb_user, emb_places, iterations=2000, learning_rate=0.01, df_val=None):
    """ 
    Computes gradient descent with momentum (0.9) for given number of iterations.
    emb_user: the trained user embedding
    emb_places: the trained places embedding
    """
    Y = create_sparse_matrix(df, emb_user.shape[0], emb_places.shape[0])
    beta = 0.9
    grad_user, grad_places = gradient(df, emb_user, emb_places)
    v_user = grad_user
    v_places = grad_places
    for i in range(iterations):
        grad_user, grad_places = gradient(df, emb_user, emb_places)
        v_user = beta*v_user + (1-beta)*grad_user
        v_places = beta*v_places + (1-beta)*grad_places
        emb_user = emb_user - learning_rate*v_user
        emb_places = emb_places - learning_rate*v_places
        if(not (i+1)%50):
            print("\niteration", i+1, ":")
            print("train mse:",  cost(df, emb_user, emb_places))
            if df_val is not None:
                print("validation mse:",  cost(df_val, emb_user, emb_places))

    return emb_user, emb_places

# calls the gradient descent prints the appropriate loss for each iteration
emb_user = create_embeddings(num_users, 3)
emb_places = create_embeddings(num_places, 3)
emb_user, emb_places = gradient_descent(travel_df, emb_user, emb_places, iterations=800, learning_rate=1)

# from matplotlib import pyplot as plt

# plt.plot(iterations, train_mse, label='train')
# plt.plot(iters, val_losses, label='validation')
# plt.xlabel('Precision')
# plt.ylabel('Recall')
# plt.title('Precision versus Recall')
# plt.show()

train_mse = cost(train_df, emb_user, emb_places)
test_mse = cost(test_df, emb_user, emb_places)
val_mse=cost(valid_df, emb_user, emb_places)
print(train_mse, val_mse,test_mse)