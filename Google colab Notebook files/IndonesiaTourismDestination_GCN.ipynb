{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "1. Mount the google drive"
      ],
      "metadata": {
        "id": "8iKeJNAJFARE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m_6WUdY8E4Xl",
        "outputId": "c45c8cc4-16bf-48d0-d11c-26e5e632b173"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. To implement Graph structure importing necessary packages"
      ],
      "metadata": {
        "id": "lTYf2HQyFeXK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install required packages.\n",
        "import os\n",
        "import torch\n",
        "os.environ['TORCH'] = torch.__version__\n",
        "print(torch.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mqP4O2PvFclL",
        "outputId": "738fe2b7-0258-4c42-f552-41c672c50574"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.12.1+cu113\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q torch-scatter -f https://data.pyg.org/whl/torch-${TORCH}.html \n",
        "!pip install -q torch-sparse -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
        "!pip install -q git+https://github.com/pyg-team/pytorch_geometric.git"
      ],
      "metadata": {
        "id": "y70updZbFFDg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U -q PyDrive\n",
        "!pip install -q git+https://github.com/snap-stanford/deepsnap.git"
      ],
      "metadata": {
        "id": "FT7Sj6w8Fx5T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **1. Datasets**"
      ],
      "metadata": {
        "id": "te98A9EqGAD9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd"
      ],
      "metadata": {
        "id": "iUdOeHTYF0jp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "user_path='/content/drive/MyDrive/project/data/user.csv'\n",
        "rating_path = '/content/drive/MyDrive/project/data/tourism_rating.csv'\n",
        "place_path = '/content/drive/MyDrive/project/data/tourism_with_id.csv'\n",
        "\n",
        "# merged1 = pd.merge(df_ratings, df_places, on='Place_Id', how='left')\n",
        "# df_user=pd.read_csv('/content/drive/MyDrive/project/data/user.csv')\n",
        "# review_path=pd.merge(merged1, df_user, on='User_Id', how='left')"
      ],
      "metadata": {
        "id": "p1PbRbVeGPMK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "IMport modules"
      ],
      "metadata": {
        "id": "uBIzybVeMC4k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "0V6QWMHuMEPB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn, optim, Tensor"
      ],
      "metadata": {
        "id": "h59BiwVHMGcs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch_geometric"
      ],
      "metadata": {
        "id": "T8kLJVfLMHKt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch_sparse import SparseTensor, matmul"
      ],
      "metadata": {
        "id": "4_3klVzSMJBz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# packages for importing the GCN model\n",
        "\n",
        "from torch_geometric.utils import structured_negative_sampling\n",
        "from torch_geometric.data import download_url, extract_zip\n",
        "from torch_geometric.nn.conv.gcn_conv import gcn_norm\n",
        "from torch_geometric.nn.conv import MessagePassing\n",
        "from torch_geometric.typing import Adj"
      ],
      "metadata": {
        "id": "_iafA60uMK3Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Load the dataset"
      ],
      "metadata": {
        "id": "R8P3M1eXGuhT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load user and review nodes\n",
        "def load_node_csv(path, index_col):\n",
        "    \"\"\"Loads csv containing node information\n",
        "\n",
        "    Args:\n",
        "        path (str): path to csv file\n",
        "        index_col (str): column name of index column\n",
        "\n",
        "    Returns:\n",
        "        dict: mapping of csv row to node id\n",
        "    \"\"\"\n",
        "    df = pd.read_csv(path, index_col=index_col)\n",
        "    mapping = {index: i for i, index in enumerate(df.index.unique())}\n",
        "    return mapping\n",
        "\n",
        "# This method basically creates a dictionary containing index for unique user or business id."
      ],
      "metadata": {
        "id": "Sq5FjfcAGigS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "user_mapping = load_node_csv(user_path, index_col='User_Id')\n",
        "place_mapping = load_node_csv(place_path, index_col='Place_Id')\n"
      ],
      "metadata": {
        "id": "MEtIJsjrG2xX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Homogenous graph with no disticntion between users and business\n",
        "# load edges between users and business\n",
        "def load_edge_csv(path, src_index_col, src_mapping, dst_index_col,dst_mapping, link_index_col,rating_threshold):\n",
        "    \"\"\"Loads csv containing edges between users and items\n",
        "\n",
        "    Args:\n",
        "        path (str): path to csv file\n",
        "        src_index_col (str): column name of users\n",
        "        src_mapping (dict): mapping between row number and user id\n",
        "        dst_index_col (str): column name of items\n",
        "        dst_mapping (dict): mapping between row number and item id\n",
        "        link_index_col (str): column name of user item interaction\n",
        "        rating_threshold (int, optional): Threshold to determine positivity of edge. Defaults to 4.\n",
        "\n",
        "    Returns:\n",
        "        torch.Tensor: 2 by N matrix containing the node ids of N user-item edges\n",
        "    \"\"\"\n",
        "    df = pd.read_csv(path)\n",
        "    edge_index = None\n",
        "    src = [src_mapping[index] for index in df[src_index_col]]\n",
        "    # list comprehension\n",
        "    # creates src list for users, dst list for items with index values\n",
        "    dst = [dst_mapping[index] for index in df[dst_index_col]]\n",
        "    \n",
        "    edge_attr = torch.from_numpy(df[link_index_col].values).view(-1, 1).to(torch.long) >= rating_threshold\n",
        "  # Assigns edge between the user and the item if the user has rated the place/business with rating >=4\n",
        "    edge_index = [[],[]]\n",
        "    for i in range(edge_attr.shape[0]):\n",
        "        if edge_attr[i]:\n",
        "            edge_index[0].append(src[i])\n",
        "            edge_index[1].append(dst[i])\n",
        "            \n",
        "    # returns a tensor object of the indices which are yet to be converted to embeddings to be given to a model\n",
        "    return torch.tensor(edge_index)"
      ],
      "metadata": {
        "id": "tzrcMLJJJ3j1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "edge_index = load_edge_csv(\n",
        "    rating_path,\n",
        "    src_index_col='User_Id',\n",
        "    src_mapping=user_mapping,\n",
        "    dst_index_col='Place_Id',\n",
        "    dst_mapping=place_mapping,\n",
        "    link_index_col='Place_Ratings',\n",
        "    rating_threshold=4,\n",
        "    # To preprocess the data to include ratings >=4 as we would like to recommend these businsess/places to users\n",
        ")"
      ],
      "metadata": {
        "id": "ZN2BThtdMVZE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "edge_index.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mhF7ihl2JV6Y",
        "outputId": "1fba9ab1-b0a0-4595-b9a7-e4b26f12d3f5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([2, 10000])"
            ]
          },
          "metadata": {},
          "execution_count": 137
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. Train/validation/test split"
      ],
      "metadata": {
        "id": "NpFF9JrKNuGv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Representing the adjacency matrix as sparsetensor as the matrix has most values as zero\n",
        "# This representation reduces memory head\n",
        "\n",
        "num_users, num_places = len(user_mapping), len(place_mapping)\n",
        "\n",
        "#shape of the edge_index ie., link between user and places nodes is flattened\n",
        "num_interactions = edge_index.shape[1]"
      ],
      "metadata": {
        "id": "pMPtEbzxNfaf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(num_users)\n",
        "print(num_places)\n",
        "print(num_interactions) # total no of edges"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RVJttylvNy-l",
        "outputId": "df429c52-a255-4c9b-a631-a2fae7408719"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "300\n",
            "437\n",
            "2021\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(num_users)\n",
        "print(num_places)\n",
        "print(num_interactions) # total no of edges"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Srhrb5tN808",
        "outputId": "245e4534-d7ef-4bf3-8732-064fb74f9899"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "300\n",
            "437\n",
            "10000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#all_indicides hold all the index values for the graph_index in an array format \n",
        "all_indices = [i for i in range(num_interactions)]\n",
        "\n",
        "# uses sklearn.model_selection.train_test_split package to split the 3 sets train, validation and test,\n",
        "# but the underlying input graph remains the same\n",
        "\n",
        "# of all the indices it splits the 20% to test_indices and its (complement) 80% to train_indicies\n",
        "train_indices, test_indices = train_test_split(all_indices, test_size=0.1, random_state=1)\n",
        "\n",
        "# of the test_indicies, it splits 10% to test_indicies and its (complement) 10% to val_indices\n",
        "val_indices, test_indices = train_test_split(test_indices, test_size=0.5, random_state=1)"
      ],
      "metadata": {
        "id": "pva-imyeOVhP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# As we are doing link/edge prediction we are splitting this edge information to 3 sets train, validation and test\n",
        "# proportion of edges are as follows 80% --> train; 10% --> test; 10% --> validation\n",
        "\n",
        "# The [:, :] stands for everything from the beginning to the end just like for lists\n",
        "\n",
        "# The below statement assigns the interaction edges which were generated for the homogenous graph to the appropriate proportion\n",
        "# to that particular\n",
        "\n",
        "# we take the edges (tuples between user and places ie., the link information) and split to test and train datasets\n",
        "\n",
        "train_edge_index = edge_index[:, train_indices]\n",
        "val_edge_index = edge_index[:, val_indices]\n",
        "test_edge_index = edge_index[:, test_indices]"
      ],
      "metadata": {
        "id": "VC9DVNW3OZwq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# convert edge indices into Sparse Tensors: https://pytorch-geometric.readthedocs.io/en/latest/notes/sparse_tensor.html\n",
        "# Here the edge_index format (interaction graph) is converted to sparsetensor (mainly used for matrices with more zero values)\n",
        "# used for GNN models, this format is given as input to the models beneath.\n",
        "\n",
        "# train_sparse_edge_index = SparseTensor(row=train_edge_index[0], col=train_edge_index[1], sparse_sizes=(\n",
        "#     num_users + num_places, num_users + num_places))\n",
        "# val_sparse_edge_index = SparseTensor(row=val_edge_index[0], col=val_edge_index[1], sparse_sizes=(\n",
        "#     num_users + num_places, num_users + num_places))\n",
        "# test_sparse_edge_index = SparseTensor(row=test_edge_index[0], col=test_edge_index[1], sparse_sizes=(\n",
        "#     num_users + num_places, num_users + num_places))"
      ],
      "metadata": {
        "id": "DP4UoN8rOcIW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_rowinput = torch.cat([train_edge_index[0], num_users + train_edge_index[1]])\n",
        "train_colinput = torch.cat([num_users + train_edge_index[1], train_edge_index[1]])\n",
        "\n",
        "val_rowinput = torch.cat([val_edge_index[0], num_users + val_edge_index[1]])\n",
        "val_colinput = torch.cat([num_users + val_edge_index[1], val_edge_index[1]])\n",
        "\n",
        "test_rowinput = torch.cat([test_edge_index[0], num_users + test_edge_index[1]])\n",
        "test_colinput = torch.cat([num_users + test_edge_index[1], test_edge_index[1]])\n",
        "\n",
        "train_sparse_edge_index = SparseTensor(row=train_rowinput, col=train_colinput, sparse_sizes=(\n",
        "    num_users + num_places, num_users + num_places))\n",
        "val_sparse_edge_index = SparseTensor(row=val_rowinput, col=val_colinput, sparse_sizes=(\n",
        "    num_users + num_places, num_users + num_places))\n",
        "test_sparse_edge_index = SparseTensor(row=test_rowinput, col=test_colinput, sparse_sizes=(\n",
        "    num_users + num_places, num_users + num_places))"
      ],
      "metadata": {
        "id": "HurFTwKBw8p7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "9. Mini Batch for postive, negative samples"
      ],
      "metadata": {
        "id": "uZ2DcvszOg99"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# function which random samples a mini-batch of positive and negative samples\n",
        "def sample_mini_batch(batch_size, edge_index):\n",
        "    \"\"\"Randomly samples indices of a minibatch given an adjacency matrix\n",
        "\n",
        "    Args:\n",
        "        batch_size (int): minibatch size\n",
        "        edge_index (torch.Tensor): 2 by N list of edges\n",
        "\n",
        "    Returns:\n",
        "        tuple: user indices, positive item indices, negative item indices\n",
        "    \"\"\"\n",
        "    edges = structured_negative_sampling(edge_index)\n",
        "    edges = torch.stack(edges, dim=0)\n",
        "    indices = random.choices(\n",
        "        [i for i in range(edges[0].shape[0])], k=batch_size)\n",
        "    batch = edges[:, indices]\n",
        "    user_indices, pos_item_indices, neg_item_indices = batch[0], batch[1], batch[2]\n",
        "    return user_indices, pos_item_indices, neg_item_indices"
      ],
      "metadata": {
        "id": "-a2bEahNOidp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. Implementing LightGCN"
      ],
      "metadata": {
        "id": "M0y4_wWuOmpv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# defines LightGCN model\n",
        "\n",
        "# The below model uses torch_geometric.nn package for defining the LightGCN model\n",
        "# The model is from https://pytorch-geometric.readthedocs.io/en/latest/notes/create_gnn.html\n",
        "\n",
        "# packages used torch_geometric.nn.conv.gcn_conv for normalization, torch_geometric.nn.conv for MessagePassing\n",
        "\n",
        "class LightGCN(MessagePassing):\n",
        "    \"\"\"LightGCN Model as proposed in https://arxiv.org/abs/2002.02126\n",
        "    \"\"\"\n",
        "    \n",
        "    # This _init_method lets the class initialize the objects (constructor)\n",
        "    #Initializes the Light GCN model\n",
        "\n",
        "    def __init__(self, num_users, num_places, embedding_dim=64, K=3, add_self_loops=False):\n",
        "        \"\"\"Initializes LightGCN Model\n",
        "\n",
        "        Args:\n",
        "            num_users (int): Number of users\n",
        "            num_places (int): Number of places/business\n",
        "            embedding_dim (int, optional): Dimensionality of embeddings. Defaults to 8.\n",
        "            K (int, optional): Number of message passing layers. Defaults to 3.\n",
        "            add_self_loops (bool, optional): Whether to add self loops for message passing. Defaults to False.\n",
        "        \"\"\"\n",
        "\n",
        "        super().__init__()\n",
        "        # refers to the parent class objects that were created above\n",
        "\n",
        "        # The PyTorch module, nn.Embedding as seen below will assign each node an embedding vector \n",
        "        # such that we have an embedding matrix Z where each row is the d-dimensional embedding vector for a node in our graph\n",
        "\n",
        "        self.num_users, self.num_places = num_users, num_places\n",
        "        # assigns the users and places/business similar to the existing original paper code\n",
        "\n",
        "        self.embedding_dim, self.K = embedding_dim, K\n",
        "        # the embedding dimesion values and no of message passing layers are defined in the init method.\n",
        "\n",
        "        self.add_self_loops = add_self_loops\n",
        "        # Here there are no self loops as it is a bipartite graph.\n",
        "        \n",
        "        # calculating the base 64 embedding vectors for the user and places(items) (upto 8)\n",
        "        # This \n",
        "        self.users_emb = nn.Embedding(\n",
        "            num_embeddings=self.num_users, embedding_dim=self.embedding_dim) # e_u^0\n",
        "\n",
        "        self.items_emb = nn.Embedding(\n",
        "            num_embeddings=self.num_places, embedding_dim=self.embedding_dim) # e_i^0\n",
        "\n",
        "        # parameter initilzation of the initial weights \n",
        "        # initalization of weights for the embedding matrix for user, place(business)\n",
        "\n",
        "\n",
        "        # torch.nn.init.normal_(tensor, mean=0.0, std=1.0) -->Fills the input Tensor with values drawn from the normal distribution \n",
        "        # setting standard deviation = 0.1 instead of 1 (overriding the normal distribution which is 1 by setting standard deviation parameters)\n",
        "        nn.init.normal_(self.users_emb.weight, std=0.1)\n",
        "        # gaussian distribution(general distribution) mean =0, variance =1 (normal distribution)\n",
        "        # setting parametes for neural network\n",
        "        nn.init.normal_(self.items_emb.weight, std=0.1)\n",
        "\n",
        "# Message passing and aggregation functions are mentioned below\n",
        "\n",
        "    def forward(self, edge_index: SparseTensor):\n",
        "        \"\"\"Forward propagation of LightGCN Model.\n",
        "\n",
        "        Args:\n",
        "            edge_index (SparseTensor): adjacency matrix\n",
        "\n",
        "        Returns:\n",
        "            tuple (Tensor): e_u_k, e_u_0, e_i_k, e_i_0\n",
        "        \"\"\"\n",
        "        # e_u_k(embeddings of the user at kth layer), e_u_0, e_i_k, e_i_0\n",
        "        # compute \\tilde{A}: symmetrically normalized adjacency matrix\n",
        "        \n",
        "        # The goal of normalization is to transform features to be on a similar scale. \n",
        "        # This improves the performance and training stability of the model\n",
        "        edge_index_norm = gcn_norm(\n",
        "            edge_index, add_self_loops=self.add_self_loops)\n",
        "        \n",
        "        # print(\"edge_index_norm is\",edge_index_norm)\n",
        "\n",
        "        # Concatenates the given sequence of seq tensors in the given dimension.\n",
        "        # concatenates the weights  \n",
        "        # [num_users*embedding_dim] size = self.users_emb.weight size\n",
        "\n",
        "        #Combining for the final embedding\n",
        "        emb_0 = torch.cat([self.users_emb.weight, self.items_emb.weight]) # E^0\n",
        "        # print(\"emb_0:\",emb_0)\n",
        "        # ipdb.set_trace()\n",
        "        # converting the item to python list, to increase the dimensions; to create a fake 1 dimension\n",
        "        embs = [emb_0]\n",
        "        # print(\"embs\",embs)\n",
        "        # (a model takes batch_size as 1st dimension, 1* dimension of item within the list --> size  )\n",
        "        emb_k = emb_0\n",
        "        # print(\"emb_k\",emb_k)\n",
        "        # to get the diffusion matrix \n",
        "        # multi-scale diffusion\n",
        "        for i in range(self.K):\n",
        "            \n",
        "            # self.propagate is for propagating the messages\n",
        "            # updating the node\n",
        "            # edge_index_norm=torch.transpose(edge_index_norm)\n",
        "            emb_k = self.propagate(edge_index_norm, x=emb_k)\n",
        "            embs.append(emb_k)\n",
        "\n",
        "        embs = torch.stack(embs, dim=1)\n",
        "        emb_final = torch.mean(embs, dim=1) # E^K\n",
        "\n",
        "        users_emb_final, items_emb_final = torch.split(\n",
        "            emb_final, [self.num_users, self.num_places]) # splits into e_u^K and e_i^K\n",
        "\n",
        "        # returns e_u^K, e_u^0, e_i^K, e_i^0\n",
        "        return users_emb_final, self.users_emb.weight, items_emb_final, self.items_emb.weight\n",
        "\n",
        "    def message(self, x_j: Tensor) -> Tensor:\n",
        "        return x_j\n",
        "\n",
        "    def message_and_aggregate(self, adj_t: SparseTensor, x: Tensor) -> Tensor:\n",
        "        # computes \\tilde{A} @ x\n",
        "        # does the matrix multiplication like in the matrix factorization\n",
        "        return matmul(adj_t, x)\n",
        "\n",
        "model = LightGCN(num_users, num_places)"
      ],
      "metadata": {
        "id": "7QAs1OB-Ok-7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Below code is for Loss function for adjusting the weights\n",
        "\n",
        "# To train the LightGCN model, we need an objective function that aligns with our goal for movie recommendation.\n",
        "# We use the Bayesian Personalized Ranking (BPR) loss,\n",
        "# which encourages observed user-item predictions to have increasingly higher values than unobserved ones\n",
        "\n",
        "def bpr_loss(users_emb_final, users_emb_0, pos_items_emb_final, pos_items_emb_0, neg_items_emb_final, neg_items_emb_0, lambda_val):\n",
        "    \"\"\"Bayesian Personalized Ranking Loss as described in https://arxiv.org/abs/1205.2618\n",
        "\n",
        "    Args:\n",
        "        users_emb_final (torch.Tensor): e_u_k\n",
        "        users_emb_0 (torch.Tensor): e_u_0\n",
        "        pos_items_emb_final (torch.Tensor): positive e_i_k\n",
        "        pos_items_emb_0 (torch.Tensor): positive e_i_0\n",
        "        neg_items_emb_final (torch.Tensor): negative e_i_k\n",
        "        neg_items_emb_0 (torch.Tensor): negative e_i_0\n",
        "        lambda_val (float): lambda value for regularization loss term\n",
        "\n",
        "    Returns:\n",
        "        torch.Tensor: scalar bpr loss value\n",
        "    \"\"\"\n",
        "    reg_loss = lambda_val * (users_emb_0.norm(2).pow(2) +\n",
        "                             pos_items_emb_0.norm(2).pow(2) +\n",
        "                             neg_items_emb_0.norm(2).pow(2)) # L2 loss\n",
        "\n",
        "    pos_scores = torch.mul(users_emb_final, pos_items_emb_final)\n",
        "    pos_scores = torch.sum(pos_scores, dim=-1) # predicted scores of positive samples\n",
        "    neg_scores = torch.mul(users_emb_final, neg_items_emb_final)\n",
        "    neg_scores = torch.sum(neg_scores, dim=-1) # predicted scores of negative samples\n",
        "\n",
        "    loss = -torch.mean(torch.nn.functional.softplus(pos_scores - neg_scores)) + reg_loss\n",
        "\n",
        "    return loss"
      ],
      "metadata": {
        "id": "vlFl9ErHOrf2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# helper function to get N_u\n",
        "def get_user_positive_items(edge_index):\n",
        "    \"\"\"Generates dictionary of positive items for each user\n",
        "\n",
        "    Args:\n",
        "        edge_index (torch.Tensor): 2 by N list of edges\n",
        "\n",
        "    Returns:\n",
        "        dict: dictionary of positive items for each user\n",
        "    \"\"\"\n",
        "    user_pos_items = {}\n",
        "    for i in range(edge_index.shape[1]):\n",
        "        user = edge_index[0][i].item()\n",
        "        item = edge_index[1][i].item()\n",
        "        if user not in user_pos_items:\n",
        "            user_pos_items[user] = []\n",
        "        user_pos_items[user].append(item)\n",
        "    return user_pos_items"
      ],
      "metadata": {
        "id": "cAKnRfT1OvTF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# computes recall@K and precision@K\n",
        "def RecallPrecision_ATk(groundTruth, r, k):\n",
        "    \"\"\"Computers recall @ k and precision @ k\n",
        "\n",
        "    Args:\n",
        "        groundTruth (list): list of lists containing highly rated items of each user\n",
        "        r (list): list of lists indicating whether each top k item recommended to each user\n",
        "            is a top k ground truth item or not\n",
        "        k (intg): determines the top k items to compute precision and recall on\n",
        "\n",
        "    Returns:\n",
        "        tuple: recall @ k, precision @ k\n",
        "    \"\"\"\n",
        "    num_correct_pred = torch.sum(r, dim=-1)  # number of correctly predicted items per user\n",
        "    # number of items liked by each user in the test set\n",
        "    user_num_liked = torch.Tensor([len(groundTruth[i])\n",
        "                                  for i in range(len(groundTruth))])\n",
        "    recall = torch.mean(num_correct_pred / user_num_liked)\n",
        "    precision = torch.mean(num_correct_pred / k)\n",
        "    return recall.item(), precision.item()"
      ],
      "metadata": {
        "id": "FpSmypPMOzlj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# computes NDCG@K\n",
        "def NDCGatK_r(groundTruth, r, k):\n",
        "    \"\"\"Computes Normalized Discounted Cumulative Gain (NDCG) @ k\n",
        "\n",
        "    Args:\n",
        "        groundTruth (list): list of lists containing highly rated items of each user\n",
        "        r (list): list of lists indicating whether each top k item recommended to each user\n",
        "            is a top k ground truth item or not\n",
        "        k (int): determines the top k items to compute ndcg on\n",
        "\n",
        "    Returns:\n",
        "        float: ndcg @ k\n",
        "    \"\"\"\n",
        "    assert len(r) == len(groundTruth)\n",
        "\n",
        "    test_matrix = torch.zeros((len(r), k))\n",
        "\n",
        "    for i, items in enumerate(groundTruth):\n",
        "        length = min(len(items), k)\n",
        "        test_matrix[i, :length] = 1\n",
        "    max_r = test_matrix\n",
        "    idcg = torch.sum(max_r * 1. / torch.log2(torch.arange(2, k + 2)), axis=1)\n",
        "    dcg = r * (1. / torch.log2(torch.arange(2, k + 2)))\n",
        "    dcg = torch.sum(dcg, axis=1)\n",
        "    idcg[idcg == 0.] = 1.\n",
        "    ndcg = dcg / idcg\n",
        "    ndcg[torch.isnan(ndcg)] = 0.\n",
        "    return torch.mean(ndcg).item()"
      ],
      "metadata": {
        "id": "0M0xNV65O2qQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# wrapper function to get evaluation metrics\n",
        "def get_metrics(model, edge_index, exclude_edge_indices, k):\n",
        "    \"\"\"Computes the evaluation metrics: recall, precision, and ndcg @ k\n",
        "\n",
        "    Args:\n",
        "        model (LighGCN): lightgcn model\n",
        "        edge_index (torch.Tensor): 2 by N list of edges for split to evaluate\n",
        "        exclude_edge_indices ([type]): 2 by N list of edges for split to discount from evaluation\n",
        "        k (int): determines the top k items to compute metrics on\n",
        "\n",
        "    Returns:\n",
        "        tuple: recall @ k, precision @ k, ndcg @ k\n",
        "    \"\"\"\n",
        "    user_embedding = model.users_emb.weight\n",
        "    item_embedding = model.items_emb.weight\n",
        "\n",
        "    # get ratings between every user and item - shape is num users x num movies\n",
        "    rating = torch.matmul(user_embedding, item_embedding.T)\n",
        "\n",
        "    for exclude_edge_index in exclude_edge_indices:\n",
        "        # gets all the positive items for each user from the edge index\n",
        "        user_pos_items = get_user_positive_items(exclude_edge_index)\n",
        "        # get coordinates of all edges to exclude\n",
        "        exclude_users = []\n",
        "        exclude_items = []\n",
        "        for user, items in user_pos_items.items():\n",
        "            exclude_users.extend([user] * len(items))\n",
        "            exclude_items.extend(items)\n",
        "\n",
        "        # set ratings of excluded edges to large negative value\n",
        "        rating[exclude_users, exclude_items] = -(1 << 10)\n",
        "\n",
        "    # get the top k recommended items for each user\n",
        "    _, top_K_items = torch.topk(rating, k=k)\n",
        "\n",
        "    # get all unique users in evaluated split\n",
        "    users = edge_index[0].unique()\n",
        "\n",
        "    test_user_pos_items = get_user_positive_items(edge_index)\n",
        "\n",
        "    # convert test user pos items dictionary into a list\n",
        "    test_user_pos_items_list = [\n",
        "        test_user_pos_items[user.item()] for user in users]\n",
        "\n",
        "    # determine the correctness of topk predictions\n",
        "    r = []\n",
        "    for user in users:\n",
        "        ground_truth_items = test_user_pos_items[user.item()]\n",
        "        label = list(map(lambda x: x in ground_truth_items, top_K_items[user]))\n",
        "        r.append(label)\n",
        "    r = torch.Tensor(np.array(r).astype('float'))\n",
        "\n",
        "    recall, precision = RecallPrecision_ATk(test_user_pos_items_list, r, k)\n",
        "    ndcg = NDCGatK_r(test_user_pos_items_list, r, k)\n",
        "\n",
        "    return recall, precision, ndcg"
      ],
      "metadata": {
        "id": "BT-vxpARO3rS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# wrapper function to evaluate model\n",
        "def evaluation(model, edge_index, sparse_edge_index, exclude_edge_indices, k, lambda_val):\n",
        "    \"\"\"Evaluates model loss and metrics including recall, precision, ndcg @ k\n",
        "\n",
        "    Args:\n",
        "        model (LighGCN): lightgcn model\n",
        "        edge_index (torch.Tensor): 2 by N list of edges for split to evaluate\n",
        "        sparse_edge_index (sparseTensor): sparse adjacency matrix for split to evaluate\n",
        "        exclude_edge_indices ([type]): 2 by N list of edges for split to discount from evaluation\n",
        "        k (int): determines the top k items to compute metrics on\n",
        "        lambda_val (float): determines lambda for bpr loss\n",
        "\n",
        "    Returns:\n",
        "        tuple: bpr loss, recall @ k, precision @ k, ndcg @ k\n",
        "    \"\"\"\n",
        "    # get embeddings\n",
        "    users_emb_final, users_emb_0, items_emb_final, items_emb_0 = model.forward(\n",
        "        sparse_edge_index)\n",
        "    edges = structured_negative_sampling(\n",
        "        edge_index)\n",
        "    user_indices, pos_item_indices, neg_item_indices = edges[0], edges[1], edges[2]\n",
        "    users_emb_final, users_emb_0 = users_emb_final[user_indices], users_emb_0[user_indices]\n",
        "    pos_items_emb_final, pos_items_emb_0 = items_emb_final[\n",
        "        pos_item_indices], items_emb_0[pos_item_indices]\n",
        "    neg_items_emb_final, neg_items_emb_0 = items_emb_final[\n",
        "        neg_item_indices], items_emb_0[neg_item_indices]\n",
        "\n",
        "    loss = bpr_loss(users_emb_final, users_emb_0, pos_items_emb_final, pos_items_emb_0,\n",
        "                    neg_items_emb_final, neg_items_emb_0, lambda_val).item()\n",
        "\n",
        "    recall, precision, ndcg = get_metrics(\n",
        "        model, edge_index, exclude_edge_indices, k)\n",
        "\n",
        "    return loss, recall, precision, ndcg"
      ],
      "metadata": {
        "id": "DX1RkfE2O62Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "r5Cw8cUBovCW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training:\n"
      ],
      "metadata": {
        "id": "ruPJ5QyYPANy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# define contants\n",
        "ITERATIONS = 20000\n",
        "BATCH_SIZE = 1024\n",
        "LR = 1e-3\n",
        "ITERS_PER_EVAL = 200\n",
        "ITERS_PER_LR_DECAY = 200\n",
        "K = 20\n",
        "LAMBDA = 1e-6"
      ],
      "metadata": {
        "id": "82DxfBi4PBNY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# setup\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device {device}.\")\n",
        "\n",
        "# moves model to gpu device\n",
        "model = model.to(device)\n",
        "\n",
        "# keeps model in training model; other model is eval mode\n",
        "model.train()\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=LR)\n",
        "scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.95)\n",
        "\n",
        "edge_index = edge_index.to(device)\n",
        "train_edge_index = train_edge_index.to(device)\n",
        "train_sparse_edge_index = train_sparse_edge_index.to(device)\n",
        "\n",
        "val_edge_index = val_edge_index.to(device)\n",
        "val_sparse_edge_index = val_sparse_edge_index.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U96TQ8HwPET1",
        "outputId": "b575331d-7aae-4929-c480-db51c57cba7c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device cpu.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_losses = []\n",
        "val_losses = []\n",
        " \n",
        "for iter in range(ITERATIONS):\n",
        "    # forward propagation\n",
        "    users_emb_final, users_emb_0, items_emb_final, items_emb_0 = model.forward(\n",
        "        train_sparse_edge_index)\n",
        "\n",
        "    # mini batching\n",
        "    user_indices, pos_item_indices, neg_item_indices = sample_mini_batch(\n",
        "        BATCH_SIZE, train_edge_index)\n",
        "    user_indices, pos_item_indices, neg_item_indices = user_indices.to(\n",
        "        device), pos_item_indices.to(device), neg_item_indices.to(device)\n",
        "    users_emb_final, users_emb_0 = users_emb_final[user_indices], users_emb_0[user_indices]\n",
        "    pos_items_emb_final, pos_items_emb_0 = items_emb_final[\n",
        "        pos_item_indices], items_emb_0[pos_item_indices]\n",
        "    neg_items_emb_final, neg_items_emb_0 = items_emb_final[\n",
        "        neg_item_indices], items_emb_0[neg_item_indices]\n",
        "\n",
        "    # loss computation\n",
        "    train_loss = bpr_loss(users_emb_final, users_emb_0, pos_items_emb_final,\n",
        "                          pos_items_emb_0, neg_items_emb_final, neg_items_emb_0, LAMBDA)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    # doing backpropogation\n",
        "    train_loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "# How many training iterations need to be run before doing evaluation -->iters_per_eval\n",
        "# Just a counter value\n",
        "    if iter % ITERS_PER_EVAL == 0:\n",
        "        model.eval()\n",
        "        val_loss, recall, precision, ndcg = evaluation(\n",
        "            model, val_edge_index, val_sparse_edge_index, [train_edge_index], K, LAMBDA)\n",
        "        print(f\"[Iteration {iter}/{ITERATIONS}] train_loss: {round(train_loss.item(), 5)}, val_loss: {round(val_loss, 5)}, val_recall@{K}: {round(recall, 5)}, val_precision@{K}: {round(precision, 5)}, val_ndcg@{K}: {round(ndcg, 5)}\")\n",
        "        train_losses.append(train_loss.item())\n",
        "        val_losses.append(val_loss)\n",
        "        model.train()\n",
        "# after ITERS_PER_LR_DECAY of no of iterations the learning rate is reduced., every iters_per_lr_decay steps.\n",
        "    if iter % ITERS_PER_LR_DECAY == 0 and iter != 0:\n",
        "        scheduler.step()\n",
        "\n",
        "iters = [iter * ITERS_PER_EVAL for iter in range(len(train_losses))]\n",
        "plt.plot(iters, train_losses, label='train')\n",
        "plt.plot(iters, val_losses, label='validation')\n",
        "plt.xlabel('iteration')\n",
        "plt.ylabel('loss')\n",
        "plt.title('training and validation loss curves')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "Yu-N7oljPGu_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "1a58dc8d-b957-431a-e8f8-2d24b2d1b822"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Iteration 0/20000] train_loss: -0.69548, val_loss: -0.72307, val_recall@20: 0.03196, val_precision@20: 0.00274, val_ndcg@20: 0.01386\n",
            "[Iteration 200/20000] train_loss: -0.79282, val_loss: -0.8323, val_recall@20: 0.03596, val_precision@20: 0.00342, val_ndcg@20: 0.01816\n",
            "[Iteration 400/20000] train_loss: -1.47072, val_loss: -1.45543, val_recall@20: 0.04737, val_precision@20: 0.00377, val_ndcg@20: 0.0178\n",
            "[Iteration 600/20000] train_loss: -3.18685, val_loss: -2.95781, val_recall@20: 0.03881, val_precision@20: 0.00308, val_ndcg@20: 0.01675\n",
            "[Iteration 800/20000] train_loss: -5.56333, val_loss: -4.84911, val_recall@20: 0.03995, val_precision@20: 0.00274, val_ndcg@20: 0.01581\n",
            "[Iteration 1000/20000] train_loss: -9.01491, val_loss: -7.49096, val_recall@20: 0.03311, val_precision@20: 0.0024, val_ndcg@20: 0.01403\n",
            "[Iteration 1200/20000] train_loss: -12.43876, val_loss: -10.36137, val_recall@20: 0.03995, val_precision@20: 0.00274, val_ndcg@20: 0.01543\n",
            "[Iteration 1400/20000] train_loss: -15.97956, val_loss: -12.89009, val_recall@20: 0.03995, val_precision@20: 0.00274, val_ndcg@20: 0.01529\n",
            "[Iteration 1600/20000] train_loss: -18.48646, val_loss: -15.64707, val_recall@20: 0.03995, val_precision@20: 0.00274, val_ndcg@20: 0.01574\n",
            "[Iteration 1800/20000] train_loss: -24.80182, val_loss: -18.93214, val_recall@20: 0.0468, val_precision@20: 0.00308, val_ndcg@20: 0.0174\n",
            "[Iteration 2000/20000] train_loss: -27.77424, val_loss: -21.88703, val_recall@20: 0.0468, val_precision@20: 0.00308, val_ndcg@20: 0.01742\n",
            "[Iteration 2200/20000] train_loss: -31.55758, val_loss: -24.91867, val_recall@20: 0.05365, val_precision@20: 0.00342, val_ndcg@20: 0.01889\n",
            "[Iteration 2400/20000] train_loss: -35.1992, val_loss: -28.62426, val_recall@20: 0.05365, val_precision@20: 0.00342, val_ndcg@20: 0.01889\n",
            "[Iteration 2600/20000] train_loss: -40.19154, val_loss: -31.13171, val_recall@20: 0.05365, val_precision@20: 0.00342, val_ndcg@20: 0.01875\n",
            "[Iteration 2800/20000] train_loss: -43.75972, val_loss: -36.1063, val_recall@20: 0.05708, val_precision@20: 0.00377, val_ndcg@20: 0.01919\n",
            "[Iteration 3000/20000] train_loss: -44.98373, val_loss: -35.94528, val_recall@20: 0.05708, val_precision@20: 0.00377, val_ndcg@20: 0.01919\n",
            "[Iteration 3200/20000] train_loss: -50.45055, val_loss: -39.48917, val_recall@20: 0.05708, val_precision@20: 0.00377, val_ndcg@20: 0.01922\n",
            "[Iteration 3400/20000] train_loss: -56.87245, val_loss: -44.34646, val_recall@20: 0.05708, val_precision@20: 0.00377, val_ndcg@20: 0.01924\n",
            "[Iteration 3600/20000] train_loss: -59.35813, val_loss: -43.54106, val_recall@20: 0.05708, val_precision@20: 0.00377, val_ndcg@20: 0.01927\n",
            "[Iteration 3800/20000] train_loss: -64.11539, val_loss: -50.04686, val_recall@20: 0.05708, val_precision@20: 0.00377, val_ndcg@20: 0.01925\n",
            "[Iteration 4000/20000] train_loss: -64.27161, val_loss: -50.57865, val_recall@20: 0.05708, val_precision@20: 0.00377, val_ndcg@20: 0.01926\n",
            "[Iteration 4200/20000] train_loss: -73.81464, val_loss: -52.57011, val_recall@20: 0.05708, val_precision@20: 0.00377, val_ndcg@20: 0.01915\n",
            "[Iteration 4400/20000] train_loss: -74.70192, val_loss: -55.72134, val_recall@20: 0.05708, val_precision@20: 0.00377, val_ndcg@20: 0.01908\n",
            "[Iteration 4600/20000] train_loss: -75.07863, val_loss: -59.80536, val_recall@20: 0.06393, val_precision@20: 0.00411, val_ndcg@20: 0.02064\n",
            "[Iteration 4800/20000] train_loss: -80.3698, val_loss: -59.94144, val_recall@20: 0.06393, val_precision@20: 0.00411, val_ndcg@20: 0.02064\n",
            "[Iteration 5000/20000] train_loss: -81.33279, val_loss: -63.09967, val_recall@20: 0.06393, val_precision@20: 0.00411, val_ndcg@20: 0.02067\n",
            "[Iteration 5200/20000] train_loss: -77.44875, val_loss: -65.52402, val_recall@20: 0.06393, val_precision@20: 0.00411, val_ndcg@20: 0.02037\n",
            "[Iteration 5400/20000] train_loss: -95.6909, val_loss: -66.57986, val_recall@20: 0.06393, val_precision@20: 0.00411, val_ndcg@20: 0.02037\n",
            "[Iteration 5600/20000] train_loss: -83.75277, val_loss: -66.29143, val_recall@20: 0.06393, val_precision@20: 0.00411, val_ndcg@20: 0.02037\n",
            "[Iteration 5800/20000] train_loss: -94.27832, val_loss: -70.21259, val_recall@20: 0.06393, val_precision@20: 0.00411, val_ndcg@20: 0.02037\n",
            "[Iteration 6000/20000] train_loss: -102.20303, val_loss: -71.21397, val_recall@20: 0.06393, val_precision@20: 0.00411, val_ndcg@20: 0.02041\n",
            "[Iteration 6200/20000] train_loss: -99.19537, val_loss: -70.32927, val_recall@20: 0.06393, val_precision@20: 0.00411, val_ndcg@20: 0.02071\n",
            "[Iteration 6400/20000] train_loss: -102.09827, val_loss: -75.81074, val_recall@20: 0.06393, val_precision@20: 0.00411, val_ndcg@20: 0.02074\n",
            "[Iteration 6600/20000] train_loss: -97.10306, val_loss: -74.0712, val_recall@20: 0.06393, val_precision@20: 0.00411, val_ndcg@20: 0.02071\n",
            "[Iteration 6800/20000] train_loss: -103.84699, val_loss: -76.26915, val_recall@20: 0.06393, val_precision@20: 0.00411, val_ndcg@20: 0.02074\n",
            "[Iteration 7000/20000] train_loss: -108.02099, val_loss: -77.45387, val_recall@20: 0.06393, val_precision@20: 0.00411, val_ndcg@20: 0.02074\n",
            "[Iteration 7200/20000] train_loss: -107.06765, val_loss: -74.26112, val_recall@20: 0.06393, val_precision@20: 0.00411, val_ndcg@20: 0.02073\n",
            "[Iteration 7400/20000] train_loss: -109.97871, val_loss: -77.56094, val_recall@20: 0.06393, val_precision@20: 0.00411, val_ndcg@20: 0.02073\n",
            "[Iteration 7600/20000] train_loss: -110.06039, val_loss: -85.9474, val_recall@20: 0.06393, val_precision@20: 0.00411, val_ndcg@20: 0.02073\n",
            "[Iteration 7800/20000] train_loss: -112.81412, val_loss: -88.33182, val_recall@20: 0.06393, val_precision@20: 0.00411, val_ndcg@20: 0.02076\n",
            "[Iteration 8000/20000] train_loss: -111.08003, val_loss: -88.69343, val_recall@20: 0.06393, val_precision@20: 0.00411, val_ndcg@20: 0.02076\n",
            "[Iteration 8200/20000] train_loss: -118.52875, val_loss: -91.84557, val_recall@20: 0.06393, val_precision@20: 0.00411, val_ndcg@20: 0.02077\n",
            "[Iteration 8400/20000] train_loss: -118.20567, val_loss: -87.96378, val_recall@20: 0.06393, val_precision@20: 0.00411, val_ndcg@20: 0.02077\n",
            "[Iteration 8600/20000] train_loss: -117.29381, val_loss: -86.83258, val_recall@20: 0.06393, val_precision@20: 0.00411, val_ndcg@20: 0.02077\n",
            "[Iteration 8800/20000] train_loss: -117.75113, val_loss: -97.29037, val_recall@20: 0.06393, val_precision@20: 0.00411, val_ndcg@20: 0.02077\n",
            "[Iteration 9000/20000] train_loss: -113.93521, val_loss: -96.72764, val_recall@20: 0.06393, val_precision@20: 0.00411, val_ndcg@20: 0.02077\n",
            "[Iteration 9200/20000] train_loss: -114.17448, val_loss: -91.01875, val_recall@20: 0.06393, val_precision@20: 0.00411, val_ndcg@20: 0.02079\n",
            "[Iteration 9400/20000] train_loss: -121.03098, val_loss: -90.46823, val_recall@20: 0.06393, val_precision@20: 0.00411, val_ndcg@20: 0.02079\n",
            "[Iteration 9600/20000] train_loss: -133.93068, val_loss: -91.89821, val_recall@20: 0.06393, val_precision@20: 0.00411, val_ndcg@20: 0.02079\n",
            "[Iteration 9800/20000] train_loss: -125.74074, val_loss: -87.01353, val_recall@20: 0.06393, val_precision@20: 0.00411, val_ndcg@20: 0.02079\n",
            "[Iteration 10000/20000] train_loss: -132.7227, val_loss: -94.20181, val_recall@20: 0.06393, val_precision@20: 0.00411, val_ndcg@20: 0.02083\n",
            "[Iteration 10200/20000] train_loss: -117.64599, val_loss: -94.31918, val_recall@20: 0.06393, val_precision@20: 0.00411, val_ndcg@20: 0.02083\n",
            "[Iteration 10400/20000] train_loss: -124.08498, val_loss: -97.88918, val_recall@20: 0.06393, val_precision@20: 0.00411, val_ndcg@20: 0.0209\n",
            "[Iteration 10600/20000] train_loss: -136.84579, val_loss: -97.0475, val_recall@20: 0.06393, val_precision@20: 0.00411, val_ndcg@20: 0.0209\n",
            "[Iteration 10800/20000] train_loss: -132.92331, val_loss: -97.48735, val_recall@20: 0.06393, val_precision@20: 0.00411, val_ndcg@20: 0.0209\n",
            "[Iteration 11000/20000] train_loss: -136.51392, val_loss: -98.09645, val_recall@20: 0.06393, val_precision@20: 0.00411, val_ndcg@20: 0.0209\n",
            "[Iteration 11200/20000] train_loss: -128.88542, val_loss: -97.26978, val_recall@20: 0.06393, val_precision@20: 0.00411, val_ndcg@20: 0.0209\n",
            "[Iteration 11400/20000] train_loss: -141.91933, val_loss: -98.12178, val_recall@20: 0.06393, val_precision@20: 0.00411, val_ndcg@20: 0.02087\n",
            "[Iteration 11600/20000] train_loss: -123.46712, val_loss: -102.20744, val_recall@20: 0.06393, val_precision@20: 0.00411, val_ndcg@20: 0.02087\n",
            "[Iteration 11800/20000] train_loss: -134.64586, val_loss: -105.08398, val_recall@20: 0.06393, val_precision@20: 0.00411, val_ndcg@20: 0.02087\n",
            "[Iteration 12000/20000] train_loss: -131.10233, val_loss: -101.81357, val_recall@20: 0.06393, val_precision@20: 0.00411, val_ndcg@20: 0.02087\n",
            "[Iteration 12200/20000] train_loss: -126.66966, val_loss: -99.06628, val_recall@20: 0.06393, val_precision@20: 0.00411, val_ndcg@20: 0.02087\n",
            "[Iteration 12400/20000] train_loss: -127.21038, val_loss: -99.03182, val_recall@20: 0.06393, val_precision@20: 0.00411, val_ndcg@20: 0.02087\n",
            "[Iteration 12600/20000] train_loss: -128.85666, val_loss: -104.84077, val_recall@20: 0.06393, val_precision@20: 0.00411, val_ndcg@20: 0.02087\n",
            "[Iteration 12800/20000] train_loss: -134.47171, val_loss: -94.24908, val_recall@20: 0.06393, val_precision@20: 0.00411, val_ndcg@20: 0.02087\n",
            "[Iteration 13000/20000] train_loss: -127.46878, val_loss: -100.81183, val_recall@20: 0.06393, val_precision@20: 0.00411, val_ndcg@20: 0.02087\n",
            "[Iteration 13200/20000] train_loss: -137.59485, val_loss: -104.65887, val_recall@20: 0.06393, val_precision@20: 0.00411, val_ndcg@20: 0.02087\n",
            "[Iteration 13400/20000] train_loss: -139.55093, val_loss: -110.47276, val_recall@20: 0.06393, val_precision@20: 0.00411, val_ndcg@20: 0.02087\n",
            "[Iteration 13600/20000] train_loss: -123.10828, val_loss: -106.21074, val_recall@20: 0.06393, val_precision@20: 0.00411, val_ndcg@20: 0.02087\n",
            "[Iteration 13800/20000] train_loss: -129.74454, val_loss: -102.59285, val_recall@20: 0.06393, val_precision@20: 0.00411, val_ndcg@20: 0.02087\n",
            "[Iteration 14000/20000] train_loss: -134.3557, val_loss: -108.14747, val_recall@20: 0.06393, val_precision@20: 0.00411, val_ndcg@20: 0.02089\n",
            "[Iteration 14200/20000] train_loss: -134.20253, val_loss: -107.75359, val_recall@20: 0.06393, val_precision@20: 0.00411, val_ndcg@20: 0.02089\n",
            "[Iteration 14400/20000] train_loss: -135.01985, val_loss: -101.58422, val_recall@20: 0.06393, val_precision@20: 0.00411, val_ndcg@20: 0.02089\n",
            "[Iteration 14600/20000] train_loss: -131.6001, val_loss: -102.92683, val_recall@20: 0.06393, val_precision@20: 0.00411, val_ndcg@20: 0.02089\n",
            "[Iteration 14800/20000] train_loss: -134.74461, val_loss: -104.54601, val_recall@20: 0.06393, val_precision@20: 0.00411, val_ndcg@20: 0.02089\n",
            "[Iteration 15000/20000] train_loss: -141.05449, val_loss: -105.63431, val_recall@20: 0.06393, val_precision@20: 0.00411, val_ndcg@20: 0.02089\n",
            "[Iteration 15200/20000] train_loss: -136.67619, val_loss: -105.92673, val_recall@20: 0.06393, val_precision@20: 0.00411, val_ndcg@20: 0.02089\n",
            "[Iteration 15400/20000] train_loss: -131.20139, val_loss: -105.62042, val_recall@20: 0.06393, val_precision@20: 0.00411, val_ndcg@20: 0.02089\n",
            "[Iteration 15600/20000] train_loss: -142.18617, val_loss: -99.6194, val_recall@20: 0.06393, val_precision@20: 0.00411, val_ndcg@20: 0.02089\n",
            "[Iteration 15800/20000] train_loss: -137.34233, val_loss: -103.8865, val_recall@20: 0.06393, val_precision@20: 0.00411, val_ndcg@20: 0.02089\n",
            "[Iteration 16000/20000] train_loss: -134.53827, val_loss: -108.86211, val_recall@20: 0.06393, val_precision@20: 0.00411, val_ndcg@20: 0.02089\n",
            "[Iteration 16200/20000] train_loss: -145.45396, val_loss: -106.97704, val_recall@20: 0.06393, val_precision@20: 0.00411, val_ndcg@20: 0.02089\n",
            "[Iteration 16400/20000] train_loss: -141.10909, val_loss: -105.67072, val_recall@20: 0.06393, val_precision@20: 0.00411, val_ndcg@20: 0.02089\n",
            "[Iteration 16600/20000] train_loss: -134.98033, val_loss: -105.15025, val_recall@20: 0.06393, val_precision@20: 0.00411, val_ndcg@20: 0.02089\n",
            "[Iteration 16800/20000] train_loss: -140.17902, val_loss: -97.55906, val_recall@20: 0.06393, val_precision@20: 0.00411, val_ndcg@20: 0.02089\n",
            "[Iteration 17000/20000] train_loss: -137.40199, val_loss: -100.25227, val_recall@20: 0.06393, val_precision@20: 0.00411, val_ndcg@20: 0.02089\n",
            "[Iteration 17200/20000] train_loss: -133.45944, val_loss: -108.51633, val_recall@20: 0.06393, val_precision@20: 0.00411, val_ndcg@20: 0.02089\n",
            "[Iteration 17400/20000] train_loss: -144.8878, val_loss: -110.73355, val_recall@20: 0.06393, val_precision@20: 0.00411, val_ndcg@20: 0.02089\n",
            "[Iteration 17600/20000] train_loss: -146.63788, val_loss: -107.61399, val_recall@20: 0.06393, val_precision@20: 0.00411, val_ndcg@20: 0.02089\n",
            "[Iteration 17800/20000] train_loss: -130.61526, val_loss: -107.79604, val_recall@20: 0.06393, val_precision@20: 0.00411, val_ndcg@20: 0.02089\n",
            "[Iteration 18000/20000] train_loss: -136.90329, val_loss: -100.08704, val_recall@20: 0.06393, val_precision@20: 0.00411, val_ndcg@20: 0.02089\n",
            "[Iteration 18200/20000] train_loss: -138.39888, val_loss: -107.19849, val_recall@20: 0.06393, val_precision@20: 0.00411, val_ndcg@20: 0.02089\n",
            "[Iteration 18400/20000] train_loss: -146.28893, val_loss: -102.4893, val_recall@20: 0.06393, val_precision@20: 0.00411, val_ndcg@20: 0.02089\n",
            "[Iteration 18600/20000] train_loss: -143.90254, val_loss: -103.57034, val_recall@20: 0.06393, val_precision@20: 0.00411, val_ndcg@20: 0.02089\n",
            "[Iteration 18800/20000] train_loss: -142.84363, val_loss: -111.65565, val_recall@20: 0.06393, val_precision@20: 0.00411, val_ndcg@20: 0.02089\n",
            "[Iteration 19000/20000] train_loss: -145.80348, val_loss: -107.1494, val_recall@20: 0.06393, val_precision@20: 0.00411, val_ndcg@20: 0.02089\n",
            "[Iteration 19200/20000] train_loss: -143.33542, val_loss: -106.36045, val_recall@20: 0.06393, val_precision@20: 0.00411, val_ndcg@20: 0.02089\n",
            "[Iteration 19400/20000] train_loss: -139.65131, val_loss: -96.52329, val_recall@20: 0.06393, val_precision@20: 0.00411, val_ndcg@20: 0.02089\n",
            "[Iteration 19600/20000] train_loss: -136.05386, val_loss: -111.24574, val_recall@20: 0.06393, val_precision@20: 0.00411, val_ndcg@20: 0.02089\n",
            "[Iteration 19800/20000] train_loss: -144.99889, val_loss: -106.2056, val_recall@20: 0.06393, val_precision@20: 0.00411, val_ndcg@20: 0.02089\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DFN0OWVhOdFu",
        "outputId": "c9df70e6-963a-40a4-84c3-6158af3a189c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LightGCN()"
            ]
          },
          "metadata": {},
          "execution_count": 114
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "user_embedding = model.users_emb.weight\n",
        "item_embedding = model.items_emb.weight"
      ],
      "metadata": {
        "id": "aSZQt7PNPzkk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rating = torch.matmul(user_embedding, item_embedding.T)"
      ],
      "metadata": {
        "id": "4LZCYMbDPhlA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rating.detach().numpy()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t1XIeOc4P5Wx",
        "outputId": "3d158ebe-1e01-434e-be4e-c7fc7a970e4d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-9.66247360e+08, -7.65189440e+08, -9.66581248e+08, ...,\n",
              "        -1.10752602e+09, -2.64044288e+08,  1.20009907e+09],\n",
              "       [-9.80696512e+08, -7.76631744e+08, -9.81034816e+08, ...,\n",
              "        -1.12408730e+09, -2.67992704e+08,  1.21804442e+09],\n",
              "       [-9.68957696e+08, -7.67336256e+08, -9.69292416e+08, ...,\n",
              "        -1.11063245e+09, -2.64784944e+08,  1.20346496e+09],\n",
              "       ...,\n",
              "       [-1.02448595e+09, -8.11310016e+08, -1.02484019e+09, ...,\n",
              "        -1.17427994e+09, -2.79959232e+08,  1.27243264e+09],\n",
              "       [-9.31388288e+08, -7.37584064e+08, -9.31710208e+08, ...,\n",
              "        -1.06756992e+09, -2.54518976e+08,  1.15680371e+09],\n",
              "       [-1.00109402e+09, -7.92785408e+08, -1.00144006e+09, ...,\n",
              "        -1.14746752e+09, -2.73566912e+08,  1.24337907e+09]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 117
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TkpBZwZ4QSaf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "K=test_edge_index.shape[1]"
      ],
      "metadata": {
        "id": "9SzwRHTW3RvT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluate on test set\n",
        "model.eval()\n",
        "# basically evaluates the model\n",
        "test_edge_index = test_edge_index.to(device)\n",
        "test_sparse_edge_index = test_sparse_edge_index.to(device)\n",
        "\n",
        "test_loss, test_recall, test_precision, test_ndcg = evaluation(\n",
        "            model, test_edge_index, test_sparse_edge_index, [train_edge_index], K, LAMBDA)\n",
        "\n",
        "print(f\"[test_loss: {round(test_loss, 5)}, test_recall@{K}: {round(test_recall, 5)}, test_precision@{K}: {round(test_precision, 5)}, test_ndcg@{K}: {round(test_ndcg, 5)}\")\n",
        "\n",
        "# evaluate on train set\n",
        "# model.eval()\n",
        "train_edge_index = train_edge_index.to(device)\n",
        "train_sparse_edge_index = train_sparse_edge_index.to(device)\n",
        "\n",
        "train_loss, train_recall, train_precision, train_ndcg = evaluation(\n",
        "            model, train_edge_index, train_sparse_edge_index, [test_edge_index, val_edge_index], K, LAMBDA)\n",
        "\n",
        "print(f\"[train_loss: {round(train_loss, 5)}, train_recall@{K}: {round(train_recall, 5)}, train_precision@{K}: {round(train_precision, 5)}, train_ndcg@{K}: {round(train_ndcg, 5)}\")\n",
        "\n",
        "# evaluate on validation set\n",
        "# model.eval()\n",
        "val_edge_index = val_edge_index.to(device)\n",
        "val_sparse_edge_index = val_sparse_edge_index.to(device)\n",
        "\n",
        "val_loss, val_recall, val_precision, val_ndcg = evaluation(\n",
        "            model, val_edge_index, val_sparse_edge_index, [train_edge_index], K, LAMBDA)\n",
        "\n",
        "print(f\"[val_loss: {round(val_loss, 5)}, val_recall@{K}: {round(val_recall, 5)}, val_precision@{K}: {round(val_precision, 5)}, val_ndcg@{K}: {round(val_ndcg, 5)}\")"
      ],
      "metadata": {
        "id": "YgnKol1h1zul",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4336755d-da2a-4fad-8545-2830ebb6810a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[test_loss: -100.72652, test_recall@20: 0.02471, test_precision@20: 0.00172, test_ndcg@20: 0.01243\n",
            "[train_loss: -140.66916, train_recall@20: 0.21182, train_precision@20: 0.13033, train_ndcg@20: 0.20539\n",
            "[val_loss: -104.12764, val_recall@20: 0.06393, val_precision@20: 0.00411, val_ndcg@20: 0.02089\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# def evaluation1(model, edge_index, sparse_edge_index, k):\n",
        "#     \"\"\"Evaluates model loss and metrics including recall, precision, ndcg @ k\n",
        "\n",
        "#     Args:\n",
        "#         model (LighGCN): lightgcn model\n",
        "#         edge_index (torch.Tensor): 2 by N list of edges for split to evaluate\n",
        "#         sparse_edge_index (sparseTensor): sparse adjacency matrix for split to evaluate\n",
        "#         exclude_edge_indices ([type]): 2 by N list of edges for split to discount from evaluation\n",
        "#         k (int): determines the top k items to compute metrics on\n",
        "#         lambda_val (float): determines lambda for bpr loss\n",
        "\n",
        "#     Returns:\n",
        "#         tuple: bpr loss, recall @ k, precision @ k, ndcg @ k\n",
        "#     \"\"\"\n",
        "#     # get embeddings\n",
        "#     users_emb_final, users_emb_0, items_emb_final, items_emb_0 = model.forward(\n",
        "#         sparse_edge_index)\n",
        "#     # changed edge_index to sparse_edge_index for edges of negative sampling\n",
        "\n",
        "\n",
        "#     recall, precision, ndcg = get_metrics1(\n",
        "#         model, edge_index, k)\n",
        "\n",
        "#     return recall, precision, ndcg\n",
        "\n",
        "# def get_metrics1(model, edge_index, k):\n",
        "#     \"\"\"Computes the evaluation metrics: recall, precision, and ndcg @ k\n",
        "\n",
        "#     Args:\n",
        "#         model (LighGCN): lightgcn model\n",
        "#         edge_index (torch.Tensor): 2 by N list of edges for split to evaluate\n",
        "#         exclude_edge_indices ([type]): 2 by N list of edges for split to discount from evaluation\n",
        "#         k (int): determines the top k items to compute metrics on\n",
        "\n",
        "#     Returns:\n",
        "#         tuple: recall @ k, precision @ k, ndcg @ k\n",
        "#     \"\"\"\n",
        "#     user_embedding = model.users_emb.weight\n",
        "#     item_embedding = model.items_emb.weight\n",
        "\n",
        "#     # get ratings between every user and item - shape is num users x num movies\n",
        "#     rating = torch.matmul(user_embedding, item_embedding.T)\n",
        "\n",
        "\n",
        "\n",
        "#     # get the top k recommended items for each user\n",
        "#     _, top_K_items = torch.topk(rating, k=k)\n",
        "\n",
        "#     # get all unique users in evaluated split\n",
        "#     users = edge_index[0].unique()\n",
        "\n",
        "#     test_user_pos_items = get_user_positive_items1(edge_index)\n",
        "#     # test_user_pos_items is a list of user with positive items\n",
        "#     # convert test user pos items dictionary into a list\n",
        "#     test_user_pos_items_list = [\n",
        "#         test_user_pos_items[user.item()] for user in users]\n",
        "\n",
        "#     # determine the correctness of topk predictions\n",
        "#     r = []\n",
        "#     for user in users:\n",
        "#         ground_truth_items = test_user_pos_items[user.item()]\n",
        "#         #ground_truth has all the items that are positive for the user in all the users\n",
        "#         label = list(map(lambda x: x in ground_truth_items, top_K_items[user]))\n",
        "#         r.append(label)\n",
        "#     r = torch.Tensor(np.array(r).astype('float'))\n",
        "#     # r has only the users for the top_k items that also present in the positives items for all users.\n",
        "#     recall, precision = RecallPrecision_ATk(test_user_pos_items_list, r, k)\n",
        "#     ndcg = NDCGatK_r(test_user_pos_items_list, r, k)\n",
        "\n",
        "#     return recall, precision, ndcg\n",
        "\n",
        "# def get_user_positive_items1(edge_index):\n",
        "#     \"\"\"Generates dictionary of positive items for each user\n",
        "\n",
        "#     Args:\n",
        "#         edge_index (torch.Tensor): 2 by N list of edges\n",
        "\n",
        "#     Returns:\n",
        "#         dict: dictionary of positive items for each user\n",
        "#     \"\"\"\n",
        "#     # basically returing all the items for that user which are present in the edge index (postive items)\n",
        "#     # if the items are not present in the edge index for that user --> considered negative item\n",
        "#     user_pos_items = {}\n",
        "#     for i in range(edge_index.shape[1]):\n",
        "#         user = edge_index[0][i].item()\n",
        "#         item = edge_index[1][i].item()\n",
        "#         # if user not in user_pos_items:\n",
        "#         user_pos_items[user] = []\n",
        "#         user_pos_items[user].append(item)\n",
        "#         # so user_pos_items is list of the user with the respective positive items\n",
        "#     return user_pos_items\n"
      ],
      "metadata": {
        "id": "sqjySenc3cy0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # evaluate on test set\n",
        "# model.eval()\n",
        "# # basically evaluates the model\n",
        "# test_edge_index = test_edge_index.to(device)\n",
        "# test_sparse_edge_index = test_sparse_edge_index.to(device)\n",
        "\n",
        "# test_recall, test_precision, test_ndcg = evaluation1(\n",
        "#             model, test_edge_index, test_sparse_edge_index,K)\n",
        "# #[train_edge_index, val_edge_index]\n",
        "# print(f\" test_recall@{K}: {round(test_recall, 5)}, test_precision@{K}: {round(test_precision, 5)}, test_ndcg@{K}: {round(test_ndcg, 5)}\")\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "98WRwYMw3ZRN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model.eval()\n",
        "# # basically evaluates the model\n",
        "# test_edge_index = test_edge_index.to(device)\n",
        "# test_sparse_edge_index = test_sparse_edge_index.to(device)\n",
        "\n",
        "# test_recall, test_precision, test_ndcg = evaluation1(\n",
        "#             model, test_edge_index, test_sparse_edge_index,K)\n",
        "# #[train_edge_index, val_edge_index]\n",
        "# print(f\" test_recall@{K}: {round(test_recall, 5)}, test_precision@{K}: {round(test_precision, 5)}, test_ndcg@{K}: {round(test_ndcg, 5)}\")"
      ],
      "metadata": {
        "id": "1YtL7ZdT4IZD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model.eval()\n",
        "# val_edge_index = val_edge_index.to(device)\n",
        "# val_sparse_edge_index = val_sparse_edge_index.to(device)\n",
        "\n",
        "# val_recall, val_precision, val_ndcg = evaluation1(\n",
        "#             model, val_edge_index, val_sparse_edge_index, K)\n",
        "# print(f\" val_recall@{K}: {round(val_recall, 5)}, val_precision@{K}: {round(val_precision, 5)}, val_ndcg@{K}: {round(val_ndcg, 5)}\")\n"
      ],
      "metadata": {
        "id": "OAQwgiJj4SOh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"# Make New Recommendatios for a Given User\"\"\"\n",
        "\n",
        "model.eval()\n",
        "df = pd.read_csv(place_path)\n",
        "Place_Name = pd.Series(df.Place_Name.values,index=df.Place_Id).to_dict()\n",
        "# business_categories = pd.Series(df.business_categories.values,index=df.business_id).to_dict()\n",
        "\n",
        "user_pos_items = get_user_positive_items(edge_index)\n",
        "\n",
        "def make_predictions(user_id,num_recs):\n",
        "    user = user_mapping[user_id]\n",
        "    e_u = model.users_emb.weight[user]\n",
        "    scores = model.items_emb.weight @ e_u\n",
        "\n",
        "    values, indices = torch.topk(scores, k=len(user_pos_items[user])+num_recs)\n",
        "    print(\"indic\",indices)\n",
        "    print(\"k\",len(user_pos_items[user]))\n",
        "\n",
        "    places = [index.cpu().item() for index in indices if index in user_pos_items[user]][:num_recs]\n",
        "    \n",
        "    print(places)\n",
        "    place_ids = [list(place_mapping.keys())[list(place_mapping.values()).index(place)] for place in places]\n",
        "    print(\"length is\",len(place_ids))\n",
        "    \n",
        "    place_name = [Place_Name[id] for id in place_ids]\n",
        "    print(place_name)\n",
        "    # categories = [business_categories[id] for id in review_ids]\n",
        "\n",
        "    print(f\"Here are some places that user {user_id} rated highly\")\n",
        "    \n",
        "    for i in range(len(places)):\n",
        "        # print(f\"city: {city[i]}, categories: {categories[i]} \")\n",
        "        print(f\"place: {place_name[i]}\")\n",
        "    print()\n",
        "\n",
        "    places = [index.cpu().item() for index in indices if index not in user_pos_items[user]][:num_recs]\n",
        "    place_ids = [list(place_mapping.keys())[list(place_mapping.values()).index(place)] for place in places]\n",
        "    place_name = [Place_Name[id] for id in place_ids]\n",
        "    # categories = [business_categories[id] for id in review_ids]\n",
        "\n",
        "    print(f\"Here are some suggested places for user {user_id}\")\n",
        "    for i in range(num_recs):\n",
        "        # print(f\"title: {city[i]}, genres: {categories[i]} \")\n",
        "        print(f\"place: {place_name[i]}\")\n",
        "USER_ID = 2\n",
        "NUM_RECS = 5\n",
        "\n",
        "make_predictions(USER_ID,NUM_RECS)"
      ],
      "metadata": {
        "id": "iWfBz03HPWwQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9ktT1ZQcL67L"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}